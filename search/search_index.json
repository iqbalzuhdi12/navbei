{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Penambangan Data Nama : Moh. Iqbal Zuhdi Husaeni NIM : 180411100051","title":"Penambangan Data"},{"location":"#penambangan-data","text":"Nama : Moh. Iqbal Zuhdi Husaeni NIM : 180411100051","title":"Penambangan Data"},{"location":"Tugas 1/","text":"Memahami Data Statistik Deskriptif Statistik deskriptif adalah metode dari pengorganisasian, penjumlahan, dan penyajian data dalam sebuah cara yang nyaman dan informatif, termasuk teknik grafik, dan teknik penghitungan. Pada sesi tulisan ini teknik grafik tidak akan dibahas, penyajian teknik ini akan dijelaskan pada sesi tulisan yang lain. Statistik deskriptif dapat mendeskripsikan data yang sedang dianalisis, tetapi tidak boleh menarik kesimpulan apapun dari data. Untuk pengambilan keputusan, kita perlukan cabang dari ilmu statistik lainnya yaitu statistik inferensia. Statistik inferensia akan dijelaskan pada sesi tulisan yang lain Ukuran-ukuran statistik deskriptif Ukuran statistik deskriptif dapat digolongkan menjadi dua kelompok, yaitu ukuran nilai tengah dan ukuran deviasi. Ukuran nilai tengah terdiri dari rata-rata ( mean ), median, dan modus. Sedangkan ukuran deviasi terdiri dari varians, simpangan baku, koefisien variasi, dan nilai jarak ( range ). Ukuran-ukuran statistik deskriptif tersebut akan dijelaskan penggunaannya baik untuk data tunggal maupun data berkelompok. Ukuran nilai tengah Rata-rata (Mean) Rata-rata ditulis dengan menggunakan simbol \u03bc (dibaca:\u201dmiu\u201d) untuk menyatakan rata-rata populasi, dan (dibaca: x bar) untuk menyatakan rata-rata sampel. Secara aljabar rata-rata dapat ditulis sebagai berikut: \u200b untuk rata-rata populasi \u200b \u200b dimana N adalah banyaknya populasi \u200b untuk rata-rata sampel \u200b \u200b dimana n adalah banyaknya sampel contoh: Dari 11 pohon pear menghasilkan buah dengan berat sbb (dlm Kg): 330 284 326 268 236 346 326 402 374 292 380 Hitunglah rata-rata produksi 11 pohon pear ? Jadi rata-rata produksi dari 11 pohon pear adalah 324 Kg. Rata-rata untuk data berkelompok . Apabila data sudah disajikan dalam data berkelompok seperti dalam bentuk tabel frekuensi dimana observasi-observasi dikelompokan kedalam kelas-kelas yang disebut frekuensi, maka rumus rata-ratanya adalah sebagai berikut: \u200b Contoh: Hitunglah rata-rata nilai statistik dari 50 mahasiswa pada Table 1 dibawah ini. \u200b \u200b Jadi perkiraan rata-rata nilai statistik 50 mahasiswa adalah 65,7. Median Ukuran nilai tengah lainnya yang mungkin dapat merupakan pilihan selain rata-rata adalah median. Jika data pada contoh produksi buah pear diurutkan dari nilai terkecil hingga ke nilai terbesar, maka nilai tengahnya adalah 326 kg artinya lima pohon pear mempunyai produksi dibawahnya dan lima pohon pear mempunyai produksi diatasnya. Nilai tengah inilah yang dikatakan median . Penentuan median bisa langsung didapat jika jumlah observasinya adalah ganjil, namun jika jumlah observasinya adalah genap maka akan didapat dua nilai tengah. Dalam situasi demikian, untuk mendapatkan mediannya yaitu dengan merata-ratakan dua nilai tengah yang didapat. Prosedur untuk mendapatkan median yaitu harus mengurutkan data dari yang terkecil hingga yang terbesar terlebih dahulu sebelum mengambil nilai tengahnya. Dengan kata lain median adalah data yang ke . Median untuk data berkelompok Untuk data yang sudah dikelompokkan dan disajikan dalam tabel frekuensi, maka mediannya dapat dicari dengan rumus sebagai berikut: Kelas median adalah kelas dimana terdapat nilai median di dalamnya. Untuk menentukan kelas median bagilah seluruh observasi dengan dua artinya 50 % dari seluruh observasi terletak sebelum median dan 50 % lainnya terletak sesudahnya. Jika kita lihat tabel frekuensi (Tabel 1) maka mediannya merupakan observasi yang ke (50/2) yaitu yang ke 25. Jumlah tiga frekuensi pertama ( f1 + f2 + f3 ) yaitu 3 + 5 + 8 = 16. Untuk mencapai 25 observasi diperlukan 9 observasi lagi. 9 observasi tersebut dapat dipenuhi dari frekuensi keempat ( f4 ) karena jumlah observasi f4 ada sebanyak 14 observasi. Jadi median terletak pada kelas keempat atau kelas (60 \u2013 69) dengan kata lain kelas keempat adalah kelas median. contoh: Hitunglah nilai median dari data kelompok pada Tabel 1. solusi: \u200b Jadi mediannya, \u200b Pertanyaan yang mungkin timbul adalah jika kita punya data aslinya, apakah nilai median yang sebenarnya adalah 66,33? jawabannya belum tentu, karena cara ini adalah cara interpolasi dimana data aslinya memang tidak diketahui, yang ada adalah data sudah dalam bentuk tabel frekuensi atau sudah dikelompokkan. Walaupun hasil interpolasi ini mungkin tidak tepat, namun cara ini memberikan hasil yang mendekati nilai median yang sebenarnya. Modus Modus dari suatu kelompok observasi adalah nilai observasi yang mempunyai frekuensi pemunculan paling banyak atau dengan kata lain yaitu nilai yang paling banyak muncul. Konsep dari modus ini berhubungan dengan kemunculan yang berulang-ulang dari suatu nilai observasi. Contoh: jika kita gunakan data produksi 11 pohon pear, maka modus produksinya adalah 326 kg. Dalam kegiatan sehari-hari, modus adalah ukuran nilai tengah yang paling jarang digunakan dibanding rata-rata atau median. Modus mungkin lebih sering digunakan pada data yang mempunyai banyak variasi dalam ukurannya, itupun untuk jumlah data yang besar. Sebagai contoh modus dari ukuran barang yang terjual sering digunakan untuk mengetahui barang yang paling disenangi konsumen. Suatu distribusi atau kelompok data mungkin tidak mempunyai modus atau mungkin mempunyai modus lebih dari satu. Distribusi yang mempunyai satu modus disebut Unimodus, yang mempunyai dua modus disebut Bimodus dan yang mempunyai modus lebih dari dua disebut Multimodus. contoh: Tentukan modus dari data dibawah ini, jika ada tentukan nilainya. a). 2, 3, 5, 7, 8. b). 2, 5, 7, 9, 9, 9, 10, 10, 11, 12. c). 2, 3, 4, 4, 4, 5, 5, 7, 7, 7, 9. solusi: Data a) tidak mempunyai modus karena semua nilai mempunyai frekuensi yang sama. Data b) mempunyai modus = 9, karena nilai observasi ini mempunyai frekuensi paling banyak. Data c) mempunyai dua modus yaitu 4 dan 7, dua nilai observasi tersebut mempunyai frekuensi palingbanyak dan sama banyak. Modus untuk data berkelompok Apabila data sudah dikelompokkan dan disajikan dalam tabel frekuensi, maka modusnya mempunyai rumus sebagai berikut: \u200b Kelas modus adalah kelas dimana terdapat nilai modus di dalamnya. Contoh: Hitunglah nilai modus dari data kelompok pada Tabel 1. solusi: Kelas modus adalah kelompok (60 \u2013 69), karena kelompok ini mempunyai frekuensi paling banyak. \u200b Ukuran dispersi Varians Dengan ukuran nilai tengah saja kita tidak akan pernah cukup untuk memberikan ringkasan karakteristik dari sebuah set data. Bagaimana sebaran observari dari nilai rata-ratanya? Apakah observasi mempunyai dispersi atau penyimpangan yang besar dari rata-ratanya? Kita biasanya memerlukan ukuran lainnya yaitu suatu ukuran tentang dispersi atau variasi didalam data. Pada kenyataannya nilai-nilai observasi suatu populasi ada yang lebih besar dari rata-rata dan ada yang lebih kecil dari rata. Informasi ini yang biasanya merupakan keterangan tambahan mengenai karakteristik dari satu set data yaitu informasi mengenai jumlah penyimpangan dalam data. Biasanya kita tertarik dengan penyimpangan nilai-nilai observasi dalam data terhadap rata-ratanya yaitu selisihnya. Rata-rata dari selisih kuadrat tersebut merupakan suatu ukuran penyimpangan yang biasa disebut dengan varians dari observasi. Simbol varians pada ukuran populasi adalah (dibaca: sigma kuadrat) dan pada ukuran sampel adalah s 2 . Simpangan baku Akar dari varians dinamakan standar deviasi atau simpangan baku. Standar deviasi merupakan ukuran simpangan yang sering digunakan dalam analisa. Nilai standar deviasi pada dasarnya menggambarkan besaran sebaran suatu kelompok data terhadap rata-ratanya atau dengan kata lain gambaran keheterogenan suatu kelompok data. Formula standar deviasi adalah sebagai berikut: \u200b Contoh: jika kita gunakan data produksi 11 pohon pear, maka varians produksinya adalah: Dari hasil perhitungan didapat varians produksi dari 11 pohon pear adalah sebesar 2.575,2 kg. sehingga standar deviasi produksinya adalah sebesar 50,75 kg. katakan kita mempunyai data produksi (dalam kg) sebanyak 10 pohon pear dengan jenis yang berbeda dengan kelompok 11 pohon pear sebelumnya, yaitu: 230 475 366 268 136 330 326 402 215 492 kelompok ini mempunyai nilai rata-rata yang sama dengan kelompok 11 pohon pear sebelumnya yaitu sebasar 324 kg. Apakah dua kelompok pohon pear tersebut mempunyai kemampuan produksi yang sama? atau dengan kata lain kelompok pohon pear mana yang lebih konsisten dalam berproduksi? Jika harus memilih jenis pohon pear mana yang lebih konsisten berproduksi, maka kita akan memilih pohon pear pada kelompok yang mempunyai nilai varians terkecil (kelompok yang lebih homogen). Varians untuk data berkelompok Formula varians untuk data berkelompok adalah sebagai berikut: \u200b contoh: kita gunakan data nilai statistik 50 mahasiswa. Koefisien variasi Standar deviasi dapat mengukur keheterogenan atau variasi suatu kelompok data. Namun jika kita ingin membandingkan dua kelompok data yang mempunyai ukuran yang berbeda, standar deviasi tidak dapat digunakan artinya standar deviasi yang lebih besar tidak selalu berarti kelompok data tersebut lebih heterogen Untuk keperluan perbandingan dua kelompok data tanpa melihat ukuran satuannya, maka dapat digunakan suatu ukuran variasi yang dinamakan koefisien variasi (CV). Rumus CV dituliskan sebagai berikut: \u200b Jika CV1 > CV2 berarti kelompok data pertama lebih bervariasi atau lebih heterogen dari pada kelompok kedua. Ukuran nilai jarak (Range) Ukuran dispersi yang paling sederhana pada suatu data numerik mungkin dengan cara menghitung selisih nilai terbesar (nilai maksimum) dengan nilai terkecil (nilai minimum). Cara ini dikenal dengan sebutan Range . Range = Nilai maksimum \u2013 Nilai minimum. Range produksi 11 pohon pear = 402 \u2013 306 = 96 Ukuran Range untuk data berkelompok Untuk data berkelompok, nilai range dihitung berdasarkan selisih antara nilai tengah kelas terakhir dengan nilai tengah pertama atau selisih batas atas kelas terakhir dengan batas bawah kelas pertama. Range = Nilai tengah kelas terakhir \u2013 Nilai tengah kelas pertama. atau Range = Bonderi atas kelas terakhir \u2013 Bonderi bawah kelas pertama. dari data nilai statistik 50 mahasiswa pada tabel 1, nilai range nya adalah: Range = 94,5 \u2013 34,5 = 60 (cara ini cenderung menghilangkan nilai ekstrim). atau Range = 99,5 \u2013 29,5 = 70. Contoh Program from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv(\"data.csv\",sep=';') df tinggibadan beratbadan usia lingkarbadan 0 159 61 39 76 1 168 57 25 72 2 157 67 39 85 3 164 65 37 76 4 169 58 37 86 5 159 58 31 86 6 161 62 38 75 7 168 66 30 81 8 165 56 34 85 9 157 60 36 82 10 162 66 33 90 11 159 59 27 76 12 152 57 38 79 13 158 63 25 79 14 158 68 26 74 15 156 61 40 89 16 170 56 31 80 17 154 65 35 82 18 169 61 37 76 19 167 55 33 71 20 151 58 40 88 21 161 67 34 78 22 151 66 34 87 23 164 70 34 77 24 160 64 37 71 25 156 58 40 90 26 166 69 27 78 27 158 67 34 80 28 169 59 25 85 29 162 64 39 81 ... ... ... ... ... 70 154 60 25 78 71 155 56 28 81 72 160 70 25 85 73 162 61 35 88 74 156 66 27 82 75 163 63 31 78 76 155 70 37 87 77 162 60 33 87 78 166 63 38 83 79 159 70 37 80 80 152 59 29 75 81 158 55 31 85 82 165 69 37 73 83 170 65 36 89 84 153 61 29 86 85 167 57 32 87 86 163 60 33 76 87 162 60 28 84 88 157 68 27 74 89 155 62 25 71 90 154 56 28 79 91 161 68 30 80 92 164 58 26 83 93 156 63 35 86 94 169 67 25 77 95 166 62 37 70 96 170 68 37 81 97 169 57 28 88 98 157 68 33 90 99 158 55 40 75 100 rows \u00d7 4 columns from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"data\"]+[df[col].count() for col in df.columns], [\"rata-rata\"]+[df[col].mean() for col in df.columns], [\"std\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min\"]+[df[col].min() for col in df.columns], [\"max\"]+[df[col].max() for col in df.columns], [\"Q1\"]+[df[col].quantile(0.25) for col in df.columns], [\"Q2\"]+[df[col].quantile(0.50) for col in df.columns], [\"Q3\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html'))) Pustaka https://digensia.wordpress.com/2012/03/15/statistik-deskriptif/","title":"Memahami Data"},{"location":"Tugas 1/#memahami-data","text":"","title":"Memahami Data"},{"location":"Tugas 1/#statistik-deskriptif","text":"Statistik deskriptif adalah metode dari pengorganisasian, penjumlahan, dan penyajian data dalam sebuah cara yang nyaman dan informatif, termasuk teknik grafik, dan teknik penghitungan. Pada sesi tulisan ini teknik grafik tidak akan dibahas, penyajian teknik ini akan dijelaskan pada sesi tulisan yang lain. Statistik deskriptif dapat mendeskripsikan data yang sedang dianalisis, tetapi tidak boleh menarik kesimpulan apapun dari data. Untuk pengambilan keputusan, kita perlukan cabang dari ilmu statistik lainnya yaitu statistik inferensia. Statistik inferensia akan dijelaskan pada sesi tulisan yang lain Ukuran-ukuran statistik deskriptif Ukuran statistik deskriptif dapat digolongkan menjadi dua kelompok, yaitu ukuran nilai tengah dan ukuran deviasi. Ukuran nilai tengah terdiri dari rata-rata ( mean ), median, dan modus. Sedangkan ukuran deviasi terdiri dari varians, simpangan baku, koefisien variasi, dan nilai jarak ( range ). Ukuran-ukuran statistik deskriptif tersebut akan dijelaskan penggunaannya baik untuk data tunggal maupun data berkelompok. Ukuran nilai tengah Rata-rata (Mean) Rata-rata ditulis dengan menggunakan simbol \u03bc (dibaca:\u201dmiu\u201d) untuk menyatakan rata-rata populasi, dan (dibaca: x bar) untuk menyatakan rata-rata sampel. Secara aljabar rata-rata dapat ditulis sebagai berikut: \u200b untuk rata-rata populasi \u200b \u200b dimana N adalah banyaknya populasi \u200b untuk rata-rata sampel \u200b \u200b dimana n adalah banyaknya sampel contoh: Dari 11 pohon pear menghasilkan buah dengan berat sbb (dlm Kg): 330 284 326 268 236 346 326 402 374 292 380 Hitunglah rata-rata produksi 11 pohon pear ? Jadi rata-rata produksi dari 11 pohon pear adalah 324 Kg. Rata-rata untuk data berkelompok . Apabila data sudah disajikan dalam data berkelompok seperti dalam bentuk tabel frekuensi dimana observasi-observasi dikelompokan kedalam kelas-kelas yang disebut frekuensi, maka rumus rata-ratanya adalah sebagai berikut: \u200b Contoh: Hitunglah rata-rata nilai statistik dari 50 mahasiswa pada Table 1 dibawah ini. \u200b \u200b Jadi perkiraan rata-rata nilai statistik 50 mahasiswa adalah 65,7. Median Ukuran nilai tengah lainnya yang mungkin dapat merupakan pilihan selain rata-rata adalah median. Jika data pada contoh produksi buah pear diurutkan dari nilai terkecil hingga ke nilai terbesar, maka nilai tengahnya adalah 326 kg artinya lima pohon pear mempunyai produksi dibawahnya dan lima pohon pear mempunyai produksi diatasnya. Nilai tengah inilah yang dikatakan median . Penentuan median bisa langsung didapat jika jumlah observasinya adalah ganjil, namun jika jumlah observasinya adalah genap maka akan didapat dua nilai tengah. Dalam situasi demikian, untuk mendapatkan mediannya yaitu dengan merata-ratakan dua nilai tengah yang didapat. Prosedur untuk mendapatkan median yaitu harus mengurutkan data dari yang terkecil hingga yang terbesar terlebih dahulu sebelum mengambil nilai tengahnya. Dengan kata lain median adalah data yang ke . Median untuk data berkelompok Untuk data yang sudah dikelompokkan dan disajikan dalam tabel frekuensi, maka mediannya dapat dicari dengan rumus sebagai berikut: Kelas median adalah kelas dimana terdapat nilai median di dalamnya. Untuk menentukan kelas median bagilah seluruh observasi dengan dua artinya 50 % dari seluruh observasi terletak sebelum median dan 50 % lainnya terletak sesudahnya. Jika kita lihat tabel frekuensi (Tabel 1) maka mediannya merupakan observasi yang ke (50/2) yaitu yang ke 25. Jumlah tiga frekuensi pertama ( f1 + f2 + f3 ) yaitu 3 + 5 + 8 = 16. Untuk mencapai 25 observasi diperlukan 9 observasi lagi. 9 observasi tersebut dapat dipenuhi dari frekuensi keempat ( f4 ) karena jumlah observasi f4 ada sebanyak 14 observasi. Jadi median terletak pada kelas keempat atau kelas (60 \u2013 69) dengan kata lain kelas keempat adalah kelas median. contoh: Hitunglah nilai median dari data kelompok pada Tabel 1. solusi: \u200b Jadi mediannya, \u200b Pertanyaan yang mungkin timbul adalah jika kita punya data aslinya, apakah nilai median yang sebenarnya adalah 66,33? jawabannya belum tentu, karena cara ini adalah cara interpolasi dimana data aslinya memang tidak diketahui, yang ada adalah data sudah dalam bentuk tabel frekuensi atau sudah dikelompokkan. Walaupun hasil interpolasi ini mungkin tidak tepat, namun cara ini memberikan hasil yang mendekati nilai median yang sebenarnya. Modus Modus dari suatu kelompok observasi adalah nilai observasi yang mempunyai frekuensi pemunculan paling banyak atau dengan kata lain yaitu nilai yang paling banyak muncul. Konsep dari modus ini berhubungan dengan kemunculan yang berulang-ulang dari suatu nilai observasi. Contoh: jika kita gunakan data produksi 11 pohon pear, maka modus produksinya adalah 326 kg. Dalam kegiatan sehari-hari, modus adalah ukuran nilai tengah yang paling jarang digunakan dibanding rata-rata atau median. Modus mungkin lebih sering digunakan pada data yang mempunyai banyak variasi dalam ukurannya, itupun untuk jumlah data yang besar. Sebagai contoh modus dari ukuran barang yang terjual sering digunakan untuk mengetahui barang yang paling disenangi konsumen. Suatu distribusi atau kelompok data mungkin tidak mempunyai modus atau mungkin mempunyai modus lebih dari satu. Distribusi yang mempunyai satu modus disebut Unimodus, yang mempunyai dua modus disebut Bimodus dan yang mempunyai modus lebih dari dua disebut Multimodus. contoh: Tentukan modus dari data dibawah ini, jika ada tentukan nilainya. a). 2, 3, 5, 7, 8. b). 2, 5, 7, 9, 9, 9, 10, 10, 11, 12. c). 2, 3, 4, 4, 4, 5, 5, 7, 7, 7, 9. solusi: Data a) tidak mempunyai modus karena semua nilai mempunyai frekuensi yang sama. Data b) mempunyai modus = 9, karena nilai observasi ini mempunyai frekuensi paling banyak. Data c) mempunyai dua modus yaitu 4 dan 7, dua nilai observasi tersebut mempunyai frekuensi palingbanyak dan sama banyak. Modus untuk data berkelompok Apabila data sudah dikelompokkan dan disajikan dalam tabel frekuensi, maka modusnya mempunyai rumus sebagai berikut: \u200b Kelas modus adalah kelas dimana terdapat nilai modus di dalamnya. Contoh: Hitunglah nilai modus dari data kelompok pada Tabel 1. solusi: Kelas modus adalah kelompok (60 \u2013 69), karena kelompok ini mempunyai frekuensi paling banyak. \u200b Ukuran dispersi Varians Dengan ukuran nilai tengah saja kita tidak akan pernah cukup untuk memberikan ringkasan karakteristik dari sebuah set data. Bagaimana sebaran observari dari nilai rata-ratanya? Apakah observasi mempunyai dispersi atau penyimpangan yang besar dari rata-ratanya? Kita biasanya memerlukan ukuran lainnya yaitu suatu ukuran tentang dispersi atau variasi didalam data. Pada kenyataannya nilai-nilai observasi suatu populasi ada yang lebih besar dari rata-rata dan ada yang lebih kecil dari rata. Informasi ini yang biasanya merupakan keterangan tambahan mengenai karakteristik dari satu set data yaitu informasi mengenai jumlah penyimpangan dalam data. Biasanya kita tertarik dengan penyimpangan nilai-nilai observasi dalam data terhadap rata-ratanya yaitu selisihnya. Rata-rata dari selisih kuadrat tersebut merupakan suatu ukuran penyimpangan yang biasa disebut dengan varians dari observasi. Simbol varians pada ukuran populasi adalah (dibaca: sigma kuadrat) dan pada ukuran sampel adalah s 2 . Simpangan baku Akar dari varians dinamakan standar deviasi atau simpangan baku. Standar deviasi merupakan ukuran simpangan yang sering digunakan dalam analisa. Nilai standar deviasi pada dasarnya menggambarkan besaran sebaran suatu kelompok data terhadap rata-ratanya atau dengan kata lain gambaran keheterogenan suatu kelompok data. Formula standar deviasi adalah sebagai berikut: \u200b Contoh: jika kita gunakan data produksi 11 pohon pear, maka varians produksinya adalah: Dari hasil perhitungan didapat varians produksi dari 11 pohon pear adalah sebesar 2.575,2 kg. sehingga standar deviasi produksinya adalah sebesar 50,75 kg. katakan kita mempunyai data produksi (dalam kg) sebanyak 10 pohon pear dengan jenis yang berbeda dengan kelompok 11 pohon pear sebelumnya, yaitu: 230 475 366 268 136 330 326 402 215 492 kelompok ini mempunyai nilai rata-rata yang sama dengan kelompok 11 pohon pear sebelumnya yaitu sebasar 324 kg. Apakah dua kelompok pohon pear tersebut mempunyai kemampuan produksi yang sama? atau dengan kata lain kelompok pohon pear mana yang lebih konsisten dalam berproduksi? Jika harus memilih jenis pohon pear mana yang lebih konsisten berproduksi, maka kita akan memilih pohon pear pada kelompok yang mempunyai nilai varians terkecil (kelompok yang lebih homogen). Varians untuk data berkelompok Formula varians untuk data berkelompok adalah sebagai berikut: \u200b contoh: kita gunakan data nilai statistik 50 mahasiswa. Koefisien variasi Standar deviasi dapat mengukur keheterogenan atau variasi suatu kelompok data. Namun jika kita ingin membandingkan dua kelompok data yang mempunyai ukuran yang berbeda, standar deviasi tidak dapat digunakan artinya standar deviasi yang lebih besar tidak selalu berarti kelompok data tersebut lebih heterogen Untuk keperluan perbandingan dua kelompok data tanpa melihat ukuran satuannya, maka dapat digunakan suatu ukuran variasi yang dinamakan koefisien variasi (CV). Rumus CV dituliskan sebagai berikut: \u200b Jika CV1 > CV2 berarti kelompok data pertama lebih bervariasi atau lebih heterogen dari pada kelompok kedua. Ukuran nilai jarak (Range) Ukuran dispersi yang paling sederhana pada suatu data numerik mungkin dengan cara menghitung selisih nilai terbesar (nilai maksimum) dengan nilai terkecil (nilai minimum). Cara ini dikenal dengan sebutan Range . Range = Nilai maksimum \u2013 Nilai minimum. Range produksi 11 pohon pear = 402 \u2013 306 = 96 Ukuran Range untuk data berkelompok Untuk data berkelompok, nilai range dihitung berdasarkan selisih antara nilai tengah kelas terakhir dengan nilai tengah pertama atau selisih batas atas kelas terakhir dengan batas bawah kelas pertama. Range = Nilai tengah kelas terakhir \u2013 Nilai tengah kelas pertama. atau Range = Bonderi atas kelas terakhir \u2013 Bonderi bawah kelas pertama. dari data nilai statistik 50 mahasiswa pada tabel 1, nilai range nya adalah: Range = 94,5 \u2013 34,5 = 60 (cara ini cenderung menghilangkan nilai ekstrim). atau Range = 99,5 \u2013 29,5 = 70.","title":"Statistik Deskriptif"},{"location":"Tugas 1/#contoh-program","text":"from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd.read_csv(\"data.csv\",sep=';') df tinggibadan beratbadan usia lingkarbadan 0 159 61 39 76 1 168 57 25 72 2 157 67 39 85 3 164 65 37 76 4 169 58 37 86 5 159 58 31 86 6 161 62 38 75 7 168 66 30 81 8 165 56 34 85 9 157 60 36 82 10 162 66 33 90 11 159 59 27 76 12 152 57 38 79 13 158 63 25 79 14 158 68 26 74 15 156 61 40 89 16 170 56 31 80 17 154 65 35 82 18 169 61 37 76 19 167 55 33 71 20 151 58 40 88 21 161 67 34 78 22 151 66 34 87 23 164 70 34 77 24 160 64 37 71 25 156 58 40 90 26 166 69 27 78 27 158 67 34 80 28 169 59 25 85 29 162 64 39 81 ... ... ... ... ... 70 154 60 25 78 71 155 56 28 81 72 160 70 25 85 73 162 61 35 88 74 156 66 27 82 75 163 63 31 78 76 155 70 37 87 77 162 60 33 87 78 166 63 38 83 79 159 70 37 80 80 152 59 29 75 81 158 55 31 85 82 165 69 37 73 83 170 65 36 89 84 153 61 29 86 85 167 57 32 87 86 163 60 33 76 87 162 60 28 84 88 157 68 27 74 89 155 62 25 71 90 154 56 28 79 91 161 68 30 80 92 164 58 26 83 93 156 63 35 86 94 169 67 25 77 95 166 62 37 70 96 170 68 37 81 97 169 57 28 88 98 157 68 33 90 99 158 55 40 75 100 rows \u00d7 4 columns from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"data\"]+[df[col].count() for col in df.columns], [\"rata-rata\"]+[df[col].mean() for col in df.columns], [\"std\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min\"]+[df[col].min() for col in df.columns], [\"max\"]+[df[col].max() for col in df.columns], [\"Q1\"]+[df[col].quantile(0.25) for col in df.columns], [\"Q2\"]+[df[col].quantile(0.50) for col in df.columns], [\"Q3\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, tablefmt='html')))","title":"Contoh Program"},{"location":"Tugas 1/#pustaka","text":"https://digensia.wordpress.com/2012/03/15/statistik-deskriptif/","title":"Pustaka"},{"location":"Tugas 2/","text":"Mengukur Jarak Data Mengukur Jarak Tipe Numerik Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ dimana m adalah bilangan riel positif dan x i dan y i adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ Euclidean distance Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Weighted euclidean distance Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i. Chord distance Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2016 x \u20162 adalah $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$ Mahalanobis distance Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ dimana S adalah matrik covariance data. Cosine measure Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$ dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn) didefinisikan dengan $$ |y|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } $$ Pearson correlation Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ , dimana \u03bc x . The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269. Overlay Metric Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x) dan ai(y) adalah nilai atribut ke i yaitu Ai dari masing masing objek x dan y, \u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana C adalah banyaknya kelas, P(c|ai(x)) adalah probabilitas bersyarat dimana kelas x adalah c dari atribut Ai, yang memiliki nilai ai(x), P(c|ai(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut Ai memiliki nilai ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f yang memiliki Mf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f untuk objek ke-i adalah xif, dan ff memiliki Mf status urutan , mewakili peringkat 1,..,Mf Ganti setiap xif dengan peringkatnya, rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $$ z _ { i f }$$ Menghitung Jarak Tipe Campuran Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana =0 - jika xif atau xjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i atau objek j) jika xif=xjf=0 dan atribut f adalah binary asymmetric, selain itu =1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu. ) dihitung bergantung pada tipenya, Jika f adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f adalah nominal atau binary, =0 jika xif=xjf, sebaliknya =1 Jika f adalah ordinal maka hitung rangking dan perlakukan zif sebagai numerik. Contoh Program import pandas as pd import itertools as it import scipy.spatial.distance as spad df = pd.read_csv('data.csv', nrows=10) data = [[round(i,2) for i in x] for x in df.values.tolist()] df = pd.DataFrame(data, columns=df.columns) df import pandas as pd import itertools as it import scipy.spatial.distance as spad columns = [\"Binary\",\"Course Instructor\",\"M=1\",\"M=2\"] distdata = list(it.combinations(data, 2)) data2 = [( d[0][1], d[1][1], spad.cityblock(d[0][1:],d[1][1:]), spad.euclidean(d[0][1:],d[1][1:])) for d in distdata] pd.DataFrame(data2,columns=columns) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur Jarak Data"},{"location":"Tugas 2/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Tugas 2/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak Tipe Numerik"},{"location":"Tugas 2/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ dimana m adalah bilangan riel positif dan x i dan y i adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance"},{"location":"Tugas 2/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"Manhattan distance"},{"location":"Tugas 2/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"Tugas 2/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"Average Distance"},{"location":"Tugas 2/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance"},{"location":"Tugas 2/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2016 x \u20162 adalah $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$","title":"Chord distance"},{"location":"Tugas 2/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ dimana S adalah matrik covariance data.","title":"Mahalanobis distance"},{"location":"Tugas 2/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$ dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn) didefinisikan dengan $$ |y|_{2}=\\sqrt{ y _ { 1 } ^ { 2 } + y _ { 2 } ^ { 2 } + \\ldots + y _ { n } ^ { 2 } } $$","title":"Cosine measure"},{"location":"Tugas 2/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ , dimana \u03bc x . The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson correlation"},{"location":"Tugas 2/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana q adalah jumlah atribut yang sama dengan 1 untuk kedua objek i dan j, r adalah jumlah atribut yang sama dengan 1 untuk objek i tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek i dan j dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"Tugas 2/#mengukur-jarak-tipe-categorical","text":"Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269.","title":"Mengukur Jarak Tipe categorical"},{"location":"Tugas 2/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x) dan ai(y) adalah nilai atribut ke i yaitu Ai dari masing masing objek x dan y, \u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"Tugas 2/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana C adalah banyaknya kelas, P(c|ai(x)) adalah probabilitas bersyarat dimana kelas x adalah c dari atribut Ai, yang memiliki nilai ai(x), P(c|ai(y)) adalah probabilitas bersyarat dimana kelas y adalah c dengan atribut Ai memiliki nilai ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"Tugas 2/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"Tugas 2/#mengukur-jarak-tipe-ordinal","text":"Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal f yang memiliki Mf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. M adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan f adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai f untuk objek ke-i adalah xif, dan ff memiliki Mf status urutan , mewakili peringkat 1,..,Mf Ganti setiap xif dengan peringkatnya, rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $$ z _ { i f }$$","title":"Mengukur Jarak Tipe Ordinal"},{"location":"Tugas 2/#menghitung-jarak-tipe-campuran","text":"Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana =0 - jika xif atau xjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek i atau objek j) jika xif=xjf=0 dan atribut f adalah binary asymmetric, selain itu =1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu. ) dihitung bergantung pada tipenya, Jika f adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika f adalah nominal atau binary, =0 jika xif=xjf, sebaliknya =1 Jika f adalah ordinal maka hitung rangking dan perlakukan zif sebagai numerik.","title":"Menghitung Jarak Tipe Campuran"},{"location":"Tugas 2/#contoh-program","text":"import pandas as pd import itertools as it import scipy.spatial.distance as spad df = pd.read_csv('data.csv', nrows=10) data = [[round(i,2) for i in x] for x in df.values.tolist()] df = pd.DataFrame(data, columns=df.columns) df import pandas as pd import itertools as it import scipy.spatial.distance as spad columns = [\"Binary\",\"Course Instructor\",\"M=1\",\"M=2\"] distdata = list(it.combinations(data, 2)) data2 = [( d[0][1], d[1][1], spad.cityblock(d[0][1:],d[1][1:]), spad.euclidean(d[0][1:],d[1][1:])) for d in distdata] pd.DataFrame(data2,columns=columns) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Contoh Program"},{"location":"Tugas 3/","text":"Seleksi Fitur Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi serta dapat meningkatkan akurasi dari algoritme klasifikasi. Metode seleksi fitur yang digunakan dalam penelitian ini adalah Information gain. Metode tersebut akan melakukan proses komputasi untuk mendapatkan atribut-atribut yang paling berpengaruh terhadap dataset. Information Gain Information Gain merupakan metode seleksi fitur paling sederhana dengan melakukan perangkingan atribut dan banyak digunakan dalam aplikasi kategorisasi teks, analisis data microarray dan analisis data citra. (Chormunge & Jena, 2016). Information Gain dapat membantu mengurangi noise yang disebabkan oleh fitur-fitur yang tidak relevan. Information Gain mendeteksi fitur-fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu. Penentuan atribut terbaik dilakukan dengan menghitung nilai entropy terlebih dahulu. Entropy merupakan ukuran ketidakpastian kelas dengan menggunakan probabilitas kejadian atau atribut tertentu. (Shaltout, et al., 2014). Rumus untuk menghitung entropy ditunjukkan pada persamaan (1). Setelah mendapatkan nilai entropy, maka perhitungan Information Gain dapat dilakukan dengan menggunakan persamaan (2). $$ E(S) = \\sum_{i=1}^c {-P_i\\log{P_i}}...(1) $$ Dengan c adalah jumlah nilai yang ada pada kelas klasifikasi dan Pi merupakan jumlah sampel untuk kelas i. $$ \\operatorname{Gain}(S, A) = \\operatorname{Entropy}(S) - \\sum_{v\\in{A}} \\frac{S_{v}}{S} E(S_{v})...(2) $$ Dengan A merupakan atribut, v adalah nilai yang mungkin untuk atribut A, Values(A) adalah himpunan nilai-nilai yang mungkin untuk A, |Sv| adalah jumlah sampel untuk nilai v, |S| merupakan jumlah seluruh sampel data dan Entropy(Sv) adalah entropy untuk sampel-sampel yang memiliki nilai v. Dengan A merupakan atribut, v adalah nilai yang mungkin untuk atribut A, Values(A) adalah himpunan nilai-nilai yang mungkin untuk A, |Sv| adalah jumlah sampel untuk nilai v, |S| merupakan jumlah seluruh sampel data dan Entropy(Sv) adalah entropy untuk sampel-sampel yang memiliki nilai v. Contoh Program from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) df = read_csv('seleksi.csv',usecols = [0,1,2,3,4], sep=';') table(df) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Mencari Entropi def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309 Mencari Gain def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']] Outlook value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain dari outlok : 0.2467498197744391 Temperature value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain dari temperature : 0.029222565658954647 Humidity value count probability high 7 0.5 normal 7 0.5 gain dari humidity : 0.15183550136234136 Windy value count probability False 8 0.571429 True 6 0.428571 gain dari windy : 0.04812703040826927 Kemudian Filter keseluruhan gain untuk memilih data yang penting table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Seleksi Fitur"},{"location":"Tugas 3/#seleksi-fitur","text":"Seleksi fitur adalah salah satu tahapan praproses klasifikasi. Seleksi fitur dilakukan dengan cara memilih fitur-fitur yang relevan yang mempengaruhi hasil klasifikasi. Seleksi fitur digunakan untuk mengurangi dimensi data dan fitur-fitur yang tidak relevan. Seleksi fitur digunakan untuk meningkatkan efektifitas dan efisiensi kinerja dari algoritma klasifikasi serta dapat meningkatkan akurasi dari algoritme klasifikasi. Metode seleksi fitur yang digunakan dalam penelitian ini adalah Information gain. Metode tersebut akan melakukan proses komputasi untuk mendapatkan atribut-atribut yang paling berpengaruh terhadap dataset.","title":"Seleksi Fitur"},{"location":"Tugas 3/#information-gain","text":"Information Gain merupakan metode seleksi fitur paling sederhana dengan melakukan perangkingan atribut dan banyak digunakan dalam aplikasi kategorisasi teks, analisis data microarray dan analisis data citra. (Chormunge & Jena, 2016). Information Gain dapat membantu mengurangi noise yang disebabkan oleh fitur-fitur yang tidak relevan. Information Gain mendeteksi fitur-fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu. Penentuan atribut terbaik dilakukan dengan menghitung nilai entropy terlebih dahulu. Entropy merupakan ukuran ketidakpastian kelas dengan menggunakan probabilitas kejadian atau atribut tertentu. (Shaltout, et al., 2014). Rumus untuk menghitung entropy ditunjukkan pada persamaan (1). Setelah mendapatkan nilai entropy, maka perhitungan Information Gain dapat dilakukan dengan menggunakan persamaan (2). $$ E(S) = \\sum_{i=1}^c {-P_i\\log{P_i}}...(1) $$ Dengan c adalah jumlah nilai yang ada pada kelas klasifikasi dan Pi merupakan jumlah sampel untuk kelas i. $$ \\operatorname{Gain}(S, A) = \\operatorname{Entropy}(S) - \\sum_{v\\in{A}} \\frac{S_{v}}{S} E(S_{v})...(2) $$ Dengan A merupakan atribut, v adalah nilai yang mungkin untuk atribut A, Values(A) adalah himpunan nilai-nilai yang mungkin untuk A, |Sv| adalah jumlah sampel untuk nilai v, |S| merupakan jumlah seluruh sampel data dan Entropy(Sv) adalah entropy untuk sampel-sampel yang memiliki nilai v. Dengan A merupakan atribut, v adalah nilai yang mungkin untuk atribut A, Values(A) adalah himpunan nilai-nilai yang mungkin untuk A, |Sv| adalah jumlah sampel untuk nilai v, |S| merupakan jumlah seluruh sampel data dan Entropy(Sv) adalah entropy untuk sampel-sampel yang memiliki nilai v.","title":"Information Gain"},{"location":"Tugas 3/#contoh-program","text":"from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) df = read_csv('seleksi.csv',usecols = [0,1,2,3,4], sep=';') table(df) outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no","title":"Contoh Program"},{"location":"Tugas 3/#mencari-entropi","text":"def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['value', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('entropy target =', entropyTarget) value count probability no 5 0.357143 yes 9 0.642857 entropy target = 0.9402859586706309","title":"Mencari Entropi"},{"location":"Tugas 3/#mencari-gain","text":"def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']]","title":"Mencari Gain"},{"location":"Tugas 3/#outlook","text":"value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain dari outlok : 0.2467498197744391","title":"Outlook"},{"location":"Tugas 3/#temperature","text":"value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain dari temperature : 0.029222565658954647","title":"Temperature"},{"location":"Tugas 3/#humidity","text":"value count probability high 7 0.5 normal 7 0.5 gain dari humidity : 0.15183550136234136","title":"Humidity"},{"location":"Tugas 3/#windy","text":"value count probability False 8 0.571429 True 6 0.428571 gain dari windy : 0.04812703040826927 Kemudian Filter keseluruhan gain untuk memilih data yang penting table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Windy"},{"location":"Tugas 4/","text":"Na\u00efve Bayes Na\u00efve Bayes merupakan sebuah pengklasifikasian probabilistik sederhana yang menghitung sekumpulan probabilitas dengan menjumlahkan frekuensi dan kombinasi nilai dari dataset yang diberikan. Algoritma mengunakan teorema Bayes dan mengasumsikan semua atribut independen atau tidak saling ketergantungan yang diberikan oleh nilai pada variabel kelas. Definisi lain mengatakan Na\u00efve Bayes merupakan pengklasifikasian dengan metode probabilitas dan statistik yang dikemukan oleh ilmuwan Inggris Thomas Bayes, yaitu memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya. Na\u00efve Bayes didasarkan pada asumsi penyederhanaan bahwa nilai atribut secara kondisional saling bebas jika diberikan nilai output. Dengan kata lain, diberikan nilai output, probabilitas mengamati secara bersama adalah produk dari probabilitas individu. Keuntungan penggunaan Naive Bayes adalah bahwa metode ini hanya membutuhkan jumlah data pelatihan (Training Data) yang kecil untuk menentukan estimasi paremeter yang diperlukan dalam proses pengklasifikasian. Naive Bayes sering bekerja jauh lebih baik dalam kebanyakan situasi dunia nyata yang kompleks dari pada yang diharapkan. Naive Bayes Classifier dinilai bekerja sangat baik dibanding dengan model classifier lainnya, yaitu Na\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya. Pre-requisites Pemahaman terhadap dasar-dasar Statistika terutama mengenai Probabilitas/Peluang Pemahaman terhadap dasar-dasar teknologi Web,HTML dan CSS Pemahaman terhadap dasar-dasar basis data/database, terutama query SQL pada MySQL/mariaDB Pemahaman terhadap dasar-dasar pemrograman PHP, terutama fungsi-fungsi koneksi database dan pengelolaan tipe data array Teorema Na\u00efve Bayes Sebelum menjelaskan Na\u00efve Bayes Classifier ini, akan dijelaskan terlebih dahulu Teorema Bayes yang menjadi dasar dari metoda tersebut. Pada Teorema Bayes , bila terdapat dua kejadian yang terpisah (misalkan X dan H ), maka Teorema Bayes dirumuskan sebagai berikut.: $$ P(H|X) = \\frac{(P(X|H)P(H))}{P(X)} $$ Keterangan Teorema Bayes sering pula dikembangkan mengingat berlakunya hukum probabilitas total, menjadi seperti berikut: $$ P(\\operatorname{C_i|D}) = \\frac{\\left( P(D|C_i)\\right)P(C_i)}{P(D)} $$ Keterangan Untuk menjelaskan Teorema Na\u00efve Bayes , perlu diketahui bahwa proses klasifikasi memerlukan sejumlah petunjuk untuk menentukan kelas apa yang cocok bagi sampel yang dianalisis tersebut. Karena itu, Teorema Bayes di atas disesuaikan sebagai berikut: Di mana Variabel C merepresentasikan kelas, sementara variabel F1 ... Fn merepresentasikan karakteristik petunjuk yang dibutuhkan untuk melakukan klasifikasi. Maka rumus tersebut menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C ( Posterior ) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior ), dikali dengan peluang kemunculan karakteristik-karakteristik sampel pada kelas C (disebut juga likelihood ), dibagi dengan peluang kemunculan karakteristik-karakteristik sampel secara global (disebut juga evidence ). Naive Bayes Classifier atau bisa di sebut sebagai multinomina naive bayes merupakan model penyederhanaan dari algoritma bayes yang cocok dalam pengklasifikasian text atau dokumen. P adalah probabilitas yang muncul. Untuk data numerik P adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ Contoh Program python iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 5.4 3.7 1.5 0.2 setosa 6.9 3.1 5.4 2.1 virginica 6.6 3 4.4 1.4 versicolor 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 7.7 3.8 6.7 2.2 virginica 6 2.2 4 1 versicolor 7.2 3.2 6 1.8 virginica 5.9 3.2 4.8 1.8 versicolor 6.3 2.7 4.9 1.8 virginica 6.3 3.3 4.7 1.6 versicolor 6.2 3.4 5.4 2.3 virginica 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 6.1 2.6 5.6 1.4 virginica 5.1 3.8 1.9 0.4 setosa 5.7 3 4.2 1.2 versicolor 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.8 2.7 5.1 1.9 virginica 4.8 3.4 1.6 0.2 setosa 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor 5.1 3.3 1.7 0.5 setosa Sampel data untuk di tes \u00b6 python test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4] Identifikasi Per Grup Class Target untuk data \u00b6 python dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.03636364 3.39090909 1.56363636 0.26363636] Sigma_s => [0.33248377 0.221154 0.15666989 0.11200649] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.4 3.7 1.5 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 5.1 3.8 1.9 0.4 setosa 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 5.1 3.3 1.7 0.5 setosa versicolor ===> Mu_s => [6.12222222 2.95555556 4.47777778 1.38888889] Sigma_s => [0.38005848 0.35394601 0.29486343 0.24720662] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.6 3 4.4 1.4 versicolor 6 2.2 4 1 versicolor 5.9 3.2 4.8 1.8 versicolor 6.3 3.3 4.7 1.6 versicolor 5.7 3 4.2 1.2 versicolor 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor virginica ===> Mu_s => [6.65 3.01 5.67 1.98] Sigma_s => [0.68677993 0.38715486 0.69610025 0.28982753] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 6.9 3.1 5.4 2.1 virginica 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 6.3 2.7 4.9 1.8 virginica 6.2 3.4 5.4 2.3 virginica 6.1 2.6 5.6 1.4 virginica 5.8 2.7 5.1 1.9 virginica 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica Hitung Probabilitas Prior dan Likehood `python def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma 2))*exp(-((v-mu) 2)/(2 (sigma *2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) ` classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 8.58066e-09 5.76458e-12 0.0526488 8.1872e-242 0.366667 versicolor 2.32391e-15 6.41354e-08 6.27963e-16 9.58684e-25 0.3 virginica 4.27211e-07 1.88775e-06 5.27627e-07 3.89569e-11 0.333333 Kesimpulan \u00b6 python Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 5.52556e-30 versicolor 2.69183e-62 setosa 7.81778e-263 python print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica ## Naive Bayes untuk Data Iris `python def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) ` sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor virginica 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica versicolor 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica versicolor 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica python corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 141 of 150 == 94.000000 % MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Na\u00efve Bayes"},{"location":"Tugas 4/#naive-bayes","text":"Na\u00efve Bayes merupakan sebuah pengklasifikasian probabilistik sederhana yang menghitung sekumpulan probabilitas dengan menjumlahkan frekuensi dan kombinasi nilai dari dataset yang diberikan. Algoritma mengunakan teorema Bayes dan mengasumsikan semua atribut independen atau tidak saling ketergantungan yang diberikan oleh nilai pada variabel kelas. Definisi lain mengatakan Na\u00efve Bayes merupakan pengklasifikasian dengan metode probabilitas dan statistik yang dikemukan oleh ilmuwan Inggris Thomas Bayes, yaitu memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya. Na\u00efve Bayes didasarkan pada asumsi penyederhanaan bahwa nilai atribut secara kondisional saling bebas jika diberikan nilai output. Dengan kata lain, diberikan nilai output, probabilitas mengamati secara bersama adalah produk dari probabilitas individu. Keuntungan penggunaan Naive Bayes adalah bahwa metode ini hanya membutuhkan jumlah data pelatihan (Training Data) yang kecil untuk menentukan estimasi paremeter yang diperlukan dalam proses pengklasifikasian. Naive Bayes sering bekerja jauh lebih baik dalam kebanyakan situasi dunia nyata yang kompleks dari pada yang diharapkan. Naive Bayes Classifier dinilai bekerja sangat baik dibanding dengan model classifier lainnya, yaitu Na\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya.","title":"Na\u00efve Bayes"},{"location":"Tugas 4/#pre-requisites","text":"Pemahaman terhadap dasar-dasar Statistika terutama mengenai Probabilitas/Peluang Pemahaman terhadap dasar-dasar teknologi Web,HTML dan CSS Pemahaman terhadap dasar-dasar basis data/database, terutama query SQL pada MySQL/mariaDB Pemahaman terhadap dasar-dasar pemrograman PHP, terutama fungsi-fungsi koneksi database dan pengelolaan tipe data array","title":"Pre-requisites"},{"location":"Tugas 4/#teorema-naive-bayes","text":"Sebelum menjelaskan Na\u00efve Bayes Classifier ini, akan dijelaskan terlebih dahulu Teorema Bayes yang menjadi dasar dari metoda tersebut. Pada Teorema Bayes , bila terdapat dua kejadian yang terpisah (misalkan X dan H ), maka Teorema Bayes dirumuskan sebagai berikut.: $$ P(H|X) = \\frac{(P(X|H)P(H))}{P(X)} $$ Keterangan Teorema Bayes sering pula dikembangkan mengingat berlakunya hukum probabilitas total, menjadi seperti berikut: $$ P(\\operatorname{C_i|D}) = \\frac{\\left( P(D|C_i)\\right)P(C_i)}{P(D)} $$ Keterangan Untuk menjelaskan Teorema Na\u00efve Bayes , perlu diketahui bahwa proses klasifikasi memerlukan sejumlah petunjuk untuk menentukan kelas apa yang cocok bagi sampel yang dianalisis tersebut. Karena itu, Teorema Bayes di atas disesuaikan sebagai berikut: Di mana Variabel C merepresentasikan kelas, sementara variabel F1 ... Fn merepresentasikan karakteristik petunjuk yang dibutuhkan untuk melakukan klasifikasi. Maka rumus tersebut menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C ( Posterior ) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior ), dikali dengan peluang kemunculan karakteristik-karakteristik sampel pada kelas C (disebut juga likelihood ), dibagi dengan peluang kemunculan karakteristik-karakteristik sampel secara global (disebut juga evidence ). Naive Bayes Classifier atau bisa di sebut sebagai multinomina naive bayes merupakan model penyederhanaan dari algoritma bayes yang cocok dalam pengklasifikasian text atau dokumen. P adalah probabilitas yang muncul. Untuk data numerik P adalah: $$ P(x=v|C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\exp\\left(-\\frac{(v-\\mu_k)^2}{2\\sigma^2_k}\\right) $$ Contoh Program python iris = datasets.load_iris() data = [list(s)+[iris.target_names[iris.target[i]]] for i,s in enumerate(iris.data)] dataset = DataFrame(data, columns=iris.feature_names+['class']).sample(frac=0.2) table(dataset) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 5.4 3.7 1.5 0.2 setosa 6.9 3.1 5.4 2.1 virginica 6.6 3 4.4 1.4 versicolor 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 7.7 3.8 6.7 2.2 virginica 6 2.2 4 1 versicolor 7.2 3.2 6 1.8 virginica 5.9 3.2 4.8 1.8 versicolor 6.3 2.7 4.9 1.8 virginica 6.3 3.3 4.7 1.6 versicolor 6.2 3.4 5.4 2.3 virginica 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 6.1 2.6 5.6 1.4 virginica 5.1 3.8 1.9 0.4 setosa 5.7 3 4.2 1.2 versicolor 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.8 2.7 5.1 1.9 virginica 4.8 3.4 1.6 0.2 setosa 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor 5.1 3.3 1.7 0.5 setosa","title":"Teorema Na\u00efve Bayes"},{"location":"Tugas 4/#sampel-data-untuk-di-tes","text":"python test = [3,5,2,4] print(\"sampel data: \", test) sampel data: [3, 5, 2, 4]","title":"Sampel data untuk di tes\u00b6"},{"location":"Tugas 4/#identifikasi-per-grup-class-target-untuk-data","text":"python dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.03636364 3.39090909 1.56363636 0.26363636] Sigma_s => [0.33248377 0.221154 0.15666989 0.11200649] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.4 3.7 1.5 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.1 3.4 1.5 0.2 setosa 5.4 3.4 1.5 0.4 setosa 5.2 3.5 1.5 0.2 setosa 4.4 3.2 1.3 0.2 setosa 5.1 3.8 1.9 0.4 setosa 5.4 3.4 1.7 0.2 setosa 4.9 3.1 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 5.1 3.3 1.7 0.5 setosa versicolor ===> Mu_s => [6.12222222 2.95555556 4.47777778 1.38888889] Sigma_s => [0.38005848 0.35394601 0.29486343 0.24720662] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.6 3 4.4 1.4 versicolor 6 2.2 4 1 versicolor 5.9 3.2 4.8 1.8 versicolor 6.3 3.3 4.7 1.6 versicolor 5.7 3 4.2 1.2 versicolor 6 3.4 4.5 1.6 versicolor 6.1 2.8 4.7 1.2 versicolor 5.7 2.9 4.2 1.3 versicolor 6.8 2.8 4.8 1.4 versicolor virginica ===> Mu_s => [6.65 3.01 5.67 1.98] Sigma_s => [0.68677993 0.38715486 0.69610025 0.28982753] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 7.7 2.6 6.9 2.3 virginica 6.9 3.1 5.4 2.1 virginica 7.7 3.8 6.7 2.2 virginica 7.2 3.2 6 1.8 virginica 6.3 2.7 4.9 1.8 virginica 6.2 3.4 5.4 2.3 virginica 6.1 2.6 5.6 1.4 virginica 5.8 2.7 5.1 1.9 virginica 6.1 3 4.9 1.8 virginica 6.5 3 5.8 2.2 virginica","title":"Identifikasi Per Grup Class Target untuk data\u00b6"},{"location":"Tugas 4/#hitung-probabilitas-prior-dan-likehood","text":"`python def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma 2))*exp(-((v-mu) 2)/(2 (sigma *2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) ` classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 8.58066e-09 5.76458e-12 0.0526488 8.1872e-242 0.366667 versicolor 2.32391e-15 6.41354e-08 6.27963e-16 9.58684e-25 0.3 virginica 4.27211e-07 1.88775e-06 5.27627e-07 3.89569e-11 0.333333","title":"Hitung Probabilitas Prior dan Likehood"},{"location":"Tugas 4/#kesimpulan","text":"python Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 5.52556e-30 versicolor 2.69183e-62 setosa 7.81778e-263 python print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica ## Naive Bayes untuk Data Iris `python def predict(sampel): priorLikehoods = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(sampel)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) products = ([[r[0], prod(r[1:])] for r in priorLikehoods]) result = DataFrame(products, columns=['class', 'probability']).sort_values('probability')[::-1] return result.values[0,0] dataset_test = DataFrame([list(d)+[predict(d[:4])] for d in data], columns=list(dataset.columns)+['predicted class (by predict())']) table(dataset_test) ` sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class predicted class (by predict()) 5.1 3.5 1.4 0.2 setosa setosa 4.9 3 1.4 0.2 setosa setosa 4.7 3.2 1.3 0.2 setosa setosa 4.6 3.1 1.5 0.2 setosa setosa 5 3.6 1.4 0.2 setosa setosa 5.4 3.9 1.7 0.4 setosa setosa 4.6 3.4 1.4 0.3 setosa setosa 5 3.4 1.5 0.2 setosa setosa 4.4 2.9 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.1 setosa setosa 5.4 3.7 1.5 0.2 setosa setosa 4.8 3.4 1.6 0.2 setosa setosa 4.8 3 1.4 0.1 setosa setosa 4.3 3 1.1 0.1 setosa setosa 5.8 4 1.2 0.2 setosa setosa 5.7 4.4 1.5 0.4 setosa setosa 5.4 3.9 1.3 0.4 setosa setosa 5.1 3.5 1.4 0.3 setosa setosa 5.7 3.8 1.7 0.3 setosa setosa 5.1 3.8 1.5 0.3 setosa setosa 5.4 3.4 1.7 0.2 setosa setosa 5.1 3.7 1.5 0.4 setosa setosa 4.6 3.6 1 0.2 setosa setosa 5.1 3.3 1.7 0.5 setosa setosa 4.8 3.4 1.9 0.2 setosa setosa 5 3 1.6 0.2 setosa setosa 5 3.4 1.6 0.4 setosa setosa 5.2 3.5 1.5 0.2 setosa setosa 5.2 3.4 1.4 0.2 setosa setosa 4.7 3.2 1.6 0.2 setosa setosa 4.8 3.1 1.6 0.2 setosa setosa 5.4 3.4 1.5 0.4 setosa setosa 5.2 4.1 1.5 0.1 setosa setosa 5.5 4.2 1.4 0.2 setosa setosa 4.9 3.1 1.5 0.2 setosa setosa 5 3.2 1.2 0.2 setosa setosa 5.5 3.5 1.3 0.2 setosa setosa 4.9 3.6 1.4 0.1 setosa setosa 4.4 3 1.3 0.2 setosa setosa 5.1 3.4 1.5 0.2 setosa setosa 5 3.5 1.3 0.3 setosa setosa 4.5 2.3 1.3 0.3 setosa setosa 4.4 3.2 1.3 0.2 setosa setosa 5 3.5 1.6 0.6 setosa setosa 5.1 3.8 1.9 0.4 setosa setosa 4.8 3 1.4 0.3 setosa setosa 5.1 3.8 1.6 0.2 setosa setosa 4.6 3.2 1.4 0.2 setosa setosa 5.3 3.7 1.5 0.2 setosa setosa 5 3.3 1.4 0.2 setosa setosa 7 3.2 4.7 1.4 versicolor versicolor 6.4 3.2 4.5 1.5 versicolor versicolor 6.9 3.1 4.9 1.5 versicolor versicolor 5.5 2.3 4 1.3 versicolor versicolor 6.5 2.8 4.6 1.5 versicolor versicolor 5.7 2.8 4.5 1.3 versicolor versicolor 6.3 3.3 4.7 1.6 versicolor versicolor 4.9 2.4 3.3 1 versicolor versicolor 6.6 2.9 4.6 1.3 versicolor versicolor 5.2 2.7 3.9 1.4 versicolor versicolor 5 2 3.5 1 versicolor versicolor 5.9 3 4.2 1.5 versicolor versicolor 6 2.2 4 1 versicolor versicolor 6.1 2.9 4.7 1.4 versicolor versicolor 5.6 2.9 3.6 1.3 versicolor versicolor 6.7 3.1 4.4 1.4 versicolor versicolor 5.6 3 4.5 1.5 versicolor versicolor 5.8 2.7 4.1 1 versicolor versicolor 6.2 2.2 4.5 1.5 versicolor versicolor 5.6 2.5 3.9 1.1 versicolor versicolor 5.9 3.2 4.8 1.8 versicolor versicolor 6.1 2.8 4 1.3 versicolor versicolor 6.3 2.5 4.9 1.5 versicolor versicolor 6.1 2.8 4.7 1.2 versicolor versicolor 6.4 2.9 4.3 1.3 versicolor versicolor 6.6 3 4.4 1.4 versicolor versicolor 6.8 2.8 4.8 1.4 versicolor versicolor 6.7 3 5 1.7 versicolor virginica 6 2.9 4.5 1.5 versicolor versicolor 5.7 2.6 3.5 1 versicolor versicolor 5.5 2.4 3.8 1.1 versicolor versicolor 5.5 2.4 3.7 1 versicolor versicolor 5.8 2.7 3.9 1.2 versicolor versicolor 6 2.7 5.1 1.6 versicolor versicolor 5.4 3 4.5 1.5 versicolor versicolor 6 3.4 4.5 1.6 versicolor versicolor 6.7 3.1 4.7 1.5 versicolor versicolor 6.3 2.3 4.4 1.3 versicolor versicolor 5.6 3 4.1 1.3 versicolor versicolor 5.5 2.5 4 1.3 versicolor versicolor 5.5 2.6 4.4 1.2 versicolor versicolor 6.1 3 4.6 1.4 versicolor versicolor 5.8 2.6 4 1.2 versicolor versicolor 5 2.3 3.3 1 versicolor versicolor 5.6 2.7 4.2 1.3 versicolor versicolor 5.7 3 4.2 1.2 versicolor versicolor 5.7 2.9 4.2 1.3 versicolor versicolor 6.2 2.9 4.3 1.3 versicolor versicolor 5.1 2.5 3 1.1 versicolor virginica 5.7 2.8 4.1 1.3 versicolor versicolor 6.3 3.3 6 2.5 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 7.1 3 5.9 2.1 virginica virginica 6.3 2.9 5.6 1.8 virginica virginica 6.5 3 5.8 2.2 virginica virginica 7.6 3 6.6 2.1 virginica virginica 4.9 2.5 4.5 1.7 virginica versicolor 7.3 2.9 6.3 1.8 virginica virginica 6.7 2.5 5.8 1.8 virginica virginica 7.2 3.6 6.1 2.5 virginica virginica 6.5 3.2 5.1 2 virginica virginica 6.4 2.7 5.3 1.9 virginica virginica 6.8 3 5.5 2.1 virginica virginica 5.7 2.5 5 2 virginica virginica 5.8 2.8 5.1 2.4 virginica virginica 6.4 3.2 5.3 2.3 virginica virginica 6.5 3 5.5 1.8 virginica virginica 7.7 3.8 6.7 2.2 virginica virginica 7.7 2.6 6.9 2.3 virginica virginica 6 2.2 5 1.5 virginica versicolor 6.9 3.2 5.7 2.3 virginica virginica 5.6 2.8 4.9 2 virginica virginica 7.7 2.8 6.7 2 virginica virginica 6.3 2.7 4.9 1.8 virginica versicolor 6.7 3.3 5.7 2.1 virginica virginica 7.2 3.2 6 1.8 virginica virginica 6.2 2.8 4.8 1.8 virginica versicolor 6.1 3 4.9 1.8 virginica versicolor 6.4 2.8 5.6 2.1 virginica virginica 7.2 3 5.8 1.6 virginica virginica 7.4 2.8 6.1 1.9 virginica virginica 7.9 3.8 6.4 2 virginica virginica 6.4 2.8 5.6 2.2 virginica virginica 6.3 2.8 5.1 1.5 virginica versicolor 6.1 2.6 5.6 1.4 virginica virginica 7.7 3 6.1 2.3 virginica virginica 6.3 3.4 5.6 2.4 virginica virginica 6.4 3.1 5.5 1.8 virginica virginica 6 3 4.8 1.8 virginica versicolor 6.9 3.1 5.4 2.1 virginica virginica 6.7 3.1 5.6 2.4 virginica virginica 6.9 3.1 5.1 2.3 virginica virginica 5.8 2.7 5.1 1.9 virginica virginica 6.8 3.2 5.9 2.3 virginica virginica 6.7 3.3 5.7 2.5 virginica virginica 6.7 3 5.2 2.3 virginica virginica 6.3 2.5 5 1.9 virginica virginica 6.5 3 5.2 2 virginica virginica 6.2 3.4 5.4 2.3 virginica virginica 5.9 3 5.1 1.8 virginica virginica python corrects = dataset_test.loc[dataset_test['class'] == dataset_test['predicted class (by predict())']].shape[0] print('Prediksi Training Bayes: %d of %d == %f %%' % (corrects, len(data), corrects / len(data) * 100)) Prediksi Training Bayes: 141 of 150 == 94.000000 % MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Kesimpulan\u00b6"}]}